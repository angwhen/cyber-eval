import numpy as np
from math import floor
import pickle
from sklearn import svm,naive_bayes,ensemble
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.utils import shuffle
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import os
import math
from scipy import spatial
import random
import matplotlib.pyplot as plt
from scipy.stats.stats import pearsonr
import scipy.stats

class Node(object):
    def __init__(self,value,next_node=None):
        self.value = value
        self.reference = next_node

class LinkedList(object):
    def __init__(self,head_node):
        self.head = head_node
    def get_head(self):
        return self.head
    def get_next(self, curr_node):
        return curr_node.next_node

def is_eof(l):
    if (len(l) == 1 and l[0] == "$eof$"):
        return True
    else:
        False
        
def get_unbalanced_train(counts,all_targets):
    counts,all_targets = shuffle(counts,all_targets,random_state = 0)
    unlab_list = []
    lab_list = []
    for sent_vect,label in zip(counts,all_targets):
        if label == 0:
            unlab_list.append(sent_vect)
        else:
            lab_list.append(sent_vect)

    counts_arr = unlab_list + lab_list
    all_targets = [0]*len(unlab_list) + [1]*len(lab_list)
    print type(counts_arr)
    print type(all_targets)

    x_train, x_test, y_train, y_test = train_test_split(counts_arr,all_targets,test_size = 0.4)
    return np.asarray(x_train),np.asarray(y_train),np.asarray(x_test),np.asarray(y_test)

def get_balanced_train(counts,all_targets):
    counts,all_targets = shuffle(counts,all_targets,random_state = 0)
    unlab_list = []
    lab_list = []
    for sent_vect,label in zip(counts,all_targets):
        if label == 0:
            unlab_list.append(sent_vect)
        else:
            lab_list.append(sent_vect)

    counts_arr = unlab_list + lab_list
    all_targets = [0]*len(unlab_list) + [1]*len(lab_list)
    #print type(counts_arr)
    #print type(all_targets)

    split_num = int(floor(len(counts_arr)*0.25))
    x_train = counts_arr[:split_num]+counts_arr[-split_num:]
    y_train  =  all_targets[:split_num]+all_targets[-split_num:]
    #print ("x_train len is %d and y_train len is %d " %(len(x_train),len(y_train)))
    x_test = counts_arr[split_num:-split_num]
    y_test  =  all_targets[split_num:-split_num]
    #print ("x_test len is %d and y_test len is %d " %(len(x_test),len(y_test)))
    #print ("a single vector is %d "%len(x_train[0]))
    return np.asarray(x_train),np.asarray(y_train),np.asarray(x_test),np.asarray(y_test)


def evaluate_model(model,x_test,y_test):
    #evaluating
    predictions = model.predict(x_test)
    acc = accuracy_score(y_test,predictions,normalize=True)
    print "accuracy of model is %f" %acc

    f1 = f1_score(y_test,predictions,average='micro')
    print "f1 of model is %f" %f1

    #confusion matrix
    cnf_matrix = confusion_matrix(y_test, predictions)
    print cnf_matrix
    return (acc,f1)

def test_model(x_train,y_train,x_test,y_test,model_type):
    #training
    if model_type == "svm":
        model = svm.SVC()
    elif model_type == "naive_bayes":
        model = naive_bayes.GaussianNB()
    elif model_type =="random_forest":
        model = ensemble.RandomForestClassifier(max_depth=50, random_state=0)
    model.fit(x_train, y_train)
   
    return evaluate_model(model,x_test,y_test)

""" methods to call from main """
def load_base_model():
    all_targets = pickle.load(open("../Preprocessing/all_labels.p", "rb"))
    all_targets = [x for x in all_targets if x != "$EOF$"]
    vectors = pickle.load(open("../Models/CV1000vectors.p","rb")) #not includes eof
    return vectors,all_targets

def test_model_cyber(vectors,all_targets):
    x_train,y_train,x_test,y_test = get_balanced_train(vectors,all_targets)
    print "NB"
    a = test_model(x_train,y_train,x_test,y_test,"naive_bayes")
    print "RF (50)"
    b = test_model(x_train,y_train,x_test,y_test,"random_forest")
    print "SVM"
    c = test_model(x_train,y_train,x_test,y_test,"svm")
    return a,b,c

def get_sents_as_linked_list(all_sents):
    start = Node(all_sents[0])
    curr = start
    for sent in all_sents[1:]:
        curr.reference = Node(sent)
        curr = curr.reference
    return LinkedList(start)
        
#should be same index other than eof, so minus one from index of vectors every time get to one
def test_distrib_orig(sents_ll,all_sents,sent_vectors,frac=10000): 
    curr1 = sents_ll.head
    curr_dists = []
    next_dists = []
    curr_sent_pairs = []
    next_sent_pairs = []
    index_dif = 0
    while (curr1!=None):
        if is_eof(curr1.value):
            index_dif = index_dif + 1
            curr1 = curr1.reference
            continue
        curr1_vector = sent_vectors[all_sents.index(curr1.value)-index_dif]
        next1 = curr1.reference
        next1_vector = sent_vectors[all_sents.index(next1.value)-index_dif]
        if is_eof(next1.value):
            curr1 = curr1.reference
            continue
        curr2 = curr1
        index_dif_inner = index_dif
        while (curr2!=None):
            if is_eof(curr2.value):
                index_dif_inner = index_dif_inner + 1
                #print index_dif_inner
                curr2 = curr2.reference
                continue
            if frac != 1 and random.randint(0,frac) != 17: #only use 1/variable
                curr2 = curr2.reference
                continue
            curr2_vector = sent_vectors[all_sents.index(curr2.value)-index_dif_inner]
            next2 = curr2.reference
            next2_vector = sent_vectors[all_sents.index(next2.value)-index_dif_inner]
            if is_eof(next1.value):
                curr2 = curr2.reference
                continue
            if (len(curr1_vector) == 0 or len(curr2_vector) ==  0 or
                len(next1_vector) == 0 or len(next2_vector) == 0):
                curr2 = curr2.reference
                continue
            #remove 0 vectors too
            if (sum(curr1_vector) == 0 or sum(curr2_vector) == 0
                or sum(next1_vector) == 0 or sum(next2_vector) == 0):
                curr2 = curr2.reference
                continue
            curr_dist = 1 - spatial.distance.cosine(curr1_vector,curr2_vector)
            curr_dists.append(curr_dist)
            next_dist =  1 - spatial.distance.cosine(next1_vector,next2_vector)
            next_dists.append(next_dist)
            curr_sent_pairs.append((curr1_vector,curr2_vector))
            next_sent_pairs.append((next1_vector,next2_vector))
            curr2 = curr2.reference
        curr1 = curr1.reference
    pearson_result = pearsonr(curr_dists,next_dists)
    print pearson_result
    return curr_dists,next_dists, curr_sent_pairs, next_sent_pairs, pearson_result[0],pearson_result[1]

def main():
    #load existing model, and then tweak
    sent_vectors,all_targets = load_base_model()
    all_sents = pickle.load(open("../Preprocessing/grouped_all_sents_no_lemma_lcase.p", "rb"))
    sents_ll = get_sents_as_linked_list(all_sents)
    test_model_cyber(next_sents,curr_targets)

main()
